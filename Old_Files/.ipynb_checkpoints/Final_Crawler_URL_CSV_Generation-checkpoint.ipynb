{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from requests.exceptions import ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_valid(url):\n",
    "   #Checks if the url is valid or not\n",
    "    parsedURL = urlparse(url)\n",
    "    return bool(parsedURL.netloc) and bool(parsedURL.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function gives all urls\n",
    "count = 0\n",
    "def get_all_urls(url):\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol (umd.edu in this case)\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            #href is empty and we don't need that a element\n",
    "            continue\n",
    "        #if the link is not absolute, make it by joining relative to the base\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        #constructing an absolute URL from parsed data\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_url_valid(href):\n",
    "            #in valid url\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            #it is already in the set, so we don't need to add\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            #it is an external link. i.e\n",
    "            # Check if it is already there \n",
    "            if href not in external_urls:\n",
    "#                 print(f\"[EXT] External link: {href}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "#         print(\".\",end=\" \")\n",
    "        global count\n",
    "        count = count+1\n",
    "        print(f\"{count}[INT] Internal link: {href}\")\n",
    "\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_urls_visited = 0\n",
    "def crawl(url, max_urls=30000):\n",
    "    #Max URL is just to decrease the time if there are a lot of pages.\n",
    "    #The following code was openly available of github and I found this\n",
    "    #idea useful to inhibit crawling time\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    try:\n",
    "        links = get_all_urls(url)\n",
    "        for link in links:\n",
    "            if total_urls_visited > max_urls:\n",
    "                break\n",
    "            crawl(link, max_urls=max_urls)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"https://umd.edu/virusinfo\"\n",
    "#Input\n",
    "# base_url = input(\"Enter the URL : \")\n",
    "base_url = \"https://www.archanaskitchen.com/\"\n",
    "internal_urls.add(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedurl = urlparse(base_url)\n",
    "print(parsedurl)\n",
    "base_url_main = parsedurl.scheme+\"://\"+parsedurl.netloc\n",
    "print(base_url_main)\n",
    "# base_url_text = base_url.split(\"//\",1)[1]\n",
    "base_url_text = parsedurl.netloc+parsedurl.path\n",
    "print(base_url_text)\n",
    "# base_url_text_domain = base_url_text.split(\"/\",1)[0]\n",
    "base_url_text_domain = parsedurl.netloc\n",
    "print(base_url_text_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(base_url)\n",
    "print(\"[+] Total External links:\", len(external_urls))\n",
    "print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "print(\"[+] Total:\", len(external_urls) + len(internal_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting List to Column\n",
    "df = DataFrame(internal_urls,columns=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrecipe = df[df['URL'].str.contains('recipe')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrecipe.to_csv('filename.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
